==================================
Data Scraping
==================================
Browser automation: Playwright (Python)
Target platform: HealthUnlocked.com
Configuration file: config.py defines paths, scraping limits, selectors, and categories.#

----------------
1. Scraping General Patterns of Co-occurrence
----------------
    Step 1: scrape_usernames_by_keyword()
        Keywords defined in keywords_initializer.py and grouped by category (e.g., Mental Health, Women's Health).
        NLTK Lemmatizer applied to extend keyword variants.
        Search performed on HealthUnlocked → extract usernames from posts.
        Limit: USERNAMES_BY_KEYWORD_LIMIT = 100 per keyword
        Stored in: usernames_by_keyword.json
    Step 2: scrape_user_profiles()
        Visit profile of each user extracted above.

        Collect:
        tags (health-related)
            demographics (age, gender, country, etc.)
            bio
            communities user has participated in (from posts/replies)
            Limit: POSTS_BY_USER_LIMIT = 50 → how many posts to check for communities

    Stores:
        user profiles: general_profiles_data.json
        global community list: all_communities.json

    Progress saved continuously, including retry logic.
    Skips users after max MAX_RETRIES = 2.

----------------
2. Scraping Community-specific Patterns
----------------
    Step 3: scrape_community_members()
        Uses the global community list.
        For each community:
            Navigate to "Most Contributors" tab.
            Collect usernames.
            Limit: PAGINATION_LIMIT = 10 pages per community

        Saves results in: members_by_comm.json

    Step 4: scrape_member_profiles()
        Visits each member's profile (within each community).
        Collects same information as general scraping.
        Saves in: profiles_by_comm_data.json

----------------
Collected Data Formats:
----------------

--> general_profiles_data.json
{
  "username": {
    "tags": [...],
    "demographics": {...},
    "bio": "...",
    "communities": [...]
  }
}

--> profiles_by_comm_data.json
{
  "/community-url": {
    "username": {
      "tags": [...],
      "demographics": {...},
      "bio": "...",
      "tag_clusters": [...]
    }
  }
}

--> all_communities.json
{
  "/asthmalunguk-lung": {
    "comm_name": "Asthma Lung UK",
    "members_count": 95123,
    "posts_count": 10023,
    "about_comm": "The UK's leading asthma & lung community..."
  },
  ...
}


----------------
Limits in config.py:
----------------
USERNAMES_BY_KEYWORD_LIMIT = 100
USER_PROFILE_LIMIT = 6 (not used)
POSTS_BY_USER_LIMIT = 50
PAGINATION_LIMIT = 10
MAX_RETRIES = 2

----------------
Logging and Error Handling:
----------------
All progress, retries, and failed usernames/communities are logged into:

scraping_stats.txt
failed_general_usernames.txt
failed_communities.txt
failed_members.txt
scrape_errors.txt


----------------
Hierarchy:
----------------
1. General Patterns:
scrape_usernames_by_keyword
scrape_user_profiles
        1.scrape_profile_data, 
        2.collect_communities_of_user
                process_tab
                        extract_community_url

                        
2. Community-specific Patterns
scrape_community_members
        extract_community_metadata
scrape_member_profiles
        scrape_profile_data

----------------
GENERAL PROFILES:
----------------
Total users: 11518
Users with tags: 8554 -> 74.27%

----------------
COMMUNITY PROFILES:
----------------
Total users: 37777
Members with tags: 15757 -> 41.71%


==================================
Demographics:
==================================

----------------
GENERAL PATTERNS: 
----------------
Look at distribution of users regarding:
    country,
    age,
    gender,
    ethnicity

Data pre-processing:
    Prefer not to say’ → **N/A** (no separate category as only 23/11 000)
    Non-binary': 7, 'My pronouns are she/her. Beyond that, I'm just me': 1, 
        'She/They/He': 1, 'She/they': 1 → belong to ‘**Other**’ category

----------------
COMMUNITY-SPECIFIC PATTERNS:
----------------
We decided not to go into demographics here.

==================================
Tag Clustering:
==================================
Create a list of all tags by extractign them from general_prifiles_data and profiles_by_comm_data files and merging.
Create a set of all tags {tag : frequency}
Total unique tags (excluding 'N/A'): 2965 (saved to all_tags)
Original plan was to look at the distribution of tags freuqencies and filter out rare ones.
E.g.ignore tags with freuqncies < 10 (60th percentil of all tags). 60th percentile frequency threshold: 9.40. 
Tags remaining after filtering: 1186 out of 2965.
Problem - too many rare tags (see stats.txt file) -> skip filtering step and consider ALL tags.

----------------
Clustering step:
----------------

    ----------------
    using word-embeddings:
    ----------------
    1. word2vec (embeddings generation) + k-means (clustering) (num_clusters = 20-40)
        Convert tags into Word2Vec-compatible format (list of lists) - Each tag as a single word sentence
        Train Word2Vec model
        Generate tag embeddings
        Apply KMeans clustering

    2. GloVe + k-means (num_clusters = 20-40)
        Load pre-trained GloVe model
        glove_model = load("glove-wiki-gigaword-100")  # 100-dimensional GloVe vectors
        Generate GloVe embeddings
        Apply KMeans clustering to GloVe embeddings

    3. Sentence Transformer (BERT) + K-Means
        Initialize transformer model: model = SentenceTransformer('all-MiniLM-L6-v2')
        Convert tags to BERT embeddings   
        Generate embeddings
        Apply KMeans clustering

    4. Sentence Transformer (BERT) + Aglomerative Clustering (n=40)

    ----------------
    using LLMs: ChatGPT (model 4o)
    ----------------
    1. defince general topics of tags (not detailed) 
        1. use ChatGPT (as part of Methodology) we dont have ground truth labels since we used general profiles & communities
        2.  Topic Modelling” instead of doing it manually
    2. clustering prompt (use part 1)
        1. “classification taks”
        2. try out: n=40, n=undefined
        3. dont really know every tags to which category as dont have domain / field knowledge
        4. include final promt in thesis! promt: see tag_clustering notebook -> 23 clusters for 2965 tags
        5. problem w/ chatGPT clustering: assign only 500 (of 3000), rest → ‘Other’ cluster
            iteratively try to recluster


==================================
Data Preprocssing / Cleaning:
==================================
Filter out users with no tags: tags: ["N/A"]

final_gender_mapping = {

    "N/A": "N/A",
    "Prefer not to say": "N/A",
    "Female": "Female",
    "Woman": "Female",
    "Male": "Male",
    "Man": "Male",
    "Others" : "Others",
    "Non-binary" : "Others",
    "My pronouns are she/her.  Beyond that, I'm just me" : "Others", 
    "She/They/He": "Others",
    "She/they" : "Others",
    
}

ethnicity_mapping = {

    "N/A": "N/A",
    "Prefer not to say": "N/A",
    
    "Latino": "Latino",
    "Hispanic / Latino / Spanish": "Latino",
    "Latino": "Latino / Hispanic",
    
    "Black": "Black",
    "Black": "Black",
    "Black": "Black",
    "Black": "Black",
    
    
    "White": "White",
    
    "Asian": "Asian",
    
    "Other ethnic group" : "Other ethnic group"
}


==================================
Create Bipartite Network:
https://networkx.org/documentation/stable/reference/algorithms/bipartite.html
==================================
Extend profile data by tag_to_cluster mapping.
   "Coachtrip": {
        "tags": [
            "Lung disease",
            "Oxygen Therapy",
            "Respiratory failure"
        ],
        "tags_clusters" [
		        "Respiratory desease", 
		        "Respiratory desease", 
		        "Respiratory desease"
        ],...}

----------------
GENERAL PATTERNS:
----------------
Load file with profiles & tag-to-cluster mapping
1. Create an edge list (user, cluster, weight)
    # user - source node
    # cluster - target node
    # weight - for frequency of co-occurrence
2. Create a bipartite graph (2 types of nodes: Users & Clusters)
    # Input: nodes (source & target) & edge list
    # Add nodes (Users & Clusters)
3. Project to cluster-cluster co-occurrence network
4. Save the projected network edge list (cluster co-occurrence)

    ----------------
    DEMOGRAPHICS:
    ----------------
    GENDER (w/o matching): 2 networks:
        male
        female
    COUNTRY:
        subgroup 1: Australia | India | Canada | Ireland (w/o matching, on whole samples)
        subgroup 2: UK | US (w/ Propensity Score Matching PSM) 
        subgroup 3: US | Canada (w/ matching)
        subgroup 4: UK | Ireland (w/o matching)
    AGE (w/o matching, on whole sample for now):
        18-30 (way less than in any group)
        30-50
        50-70
        70+
    ETHNICITY (w/o matching, on whole sample for now):
        white
        asian
        black
        latino
    
    Propensity Matching Score (PSM):
        e.g. UK (4000 users) vs. US (2000 users)

        Load file with profiles & tag-to-cluster mapping
        Extract relevant user info (age, gender) into rows (one row for each user)
        Filter based on countries (or ethnicity) of interest
        Remove rows with no age and gender
        Convert age to numeric type
        Remove rows where age could not be converted

        use from psmpy import PsmPy, from psmpy.functions import cohenD
        
        Encode gender
        Create df for PSM df_filtered[["user", "country", "age", "gender_encoded"]]
        Define treatment variable (USA = 1, UK = 0)
        Initialize PSM model (with PsmPy)
        Estimate propensity scores via logistic regression
        Nearest neighbor matching 1:1
        Get matched samples
        Extract IDs of matches users (separately for UK and US)
        Save to JSON file


----------------
COMMUNITY-SPECIFIC PATTERNS:
----------------
Load file with profiles by community & tag-to-cluster mapping
Create for EACH community a SEPARATE network
"user", "cluster", "weight", "comm_url"
the same as above in GENERAL PATTERNS

==================================
Network Analysis
==================================

----------------
GENERAL PATTERNS:
----------------
for all general profiles:
    "label": "cluster_co-occurrence_edges",
        "nodes": 23,
        "edges": 253,
        "density": 1.0,
        "average_degree": 22.0,
        "average_weighted_degree": 2729.7391304347825,
        "average_clustering": 0.041655589920637864,
        "num_components": 1,
        "largest_component_size": 23,
        "diameter": 1,
        "modularity": 0.04978913381305847,
        "top5_degree_centrality",
        "top5_betweenness_centrality",
        "top5_cluster_pairs_by_weight"

    ----------------
    DEMOGRAPHICS:
    ----------------
    for each demographic subgroup, get values (nodes, edges, etc)


----------------
COMMUNITY-SPECIFIC PATTERNS:
----------------
1. helps not to look into EACH comm separately but top occurrences ACROSS comm
    23 clusters → for each cluster we calculate % of its appearance in the top 1 pairs (bar plot) e.g. Reproductive Health
        do this for each position from “top 5 heaviest” for each comm
            1. is Reprod health is in top 1 of X comm?
            2. is Reprod health is in top 2 of X comm?
            3. is Reprod health is in top 3 of X comm?
            4. etc. top 4 /5

2. (do the opposite to 1.)
- for each cluster find top N (N=5,10) communities they are coming (appearing) from
- in how many (in which comm) comm the cluster “Mental” was part of top 5 clusters? dont care about rank here
- cluster pairs are more important for generla profile, not comm
    - in Discussion: if i see some intersting cluster pairs in comm