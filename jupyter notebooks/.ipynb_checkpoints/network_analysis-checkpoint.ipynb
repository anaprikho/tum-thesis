{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "401a9ffd-578e-4f13-b70f-bc884a6cefff",
   "metadata": {},
   "source": [
    "# 1. Network Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f642687-9bfb-46f7-a3e8-4b7d16ff075d",
   "metadata": {},
   "source": [
    "- How connected is the network?\n",
    "\n",
    "- Which clusters are most central?\n",
    "\n",
    "- Are there isolated topics?\n",
    "\n",
    "- Are there groups of clusters that stick together (communities)?\n",
    "\n",
    "- Is there a difference between the UK and US networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de344794-496b-47fc-9359-df6fb25a466e",
   "metadata": {},
   "source": [
    "density, \n",
    "\n",
    "size, etc (basics), \n",
    "\n",
    "avg degree (weighted), \n",
    "\n",
    "clustering co-efficient, \n",
    "\n",
    "top-nodes for each centrality measure (falseness, degree centrality) for each nw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3349511-4d74-4de9-a6b0-70090ed67c6c",
   "metadata": {},
   "source": [
    "pairs: e.g \"Depression\" cluster for female connected to \"Reproduction Health\" cluster vs.  (what is for male?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a16d457-4970-4b5c-b7f0-472025b1a132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>nodes</th>\n",
       "      <th>edges</th>\n",
       "      <th>density</th>\n",
       "      <th>average_degree</th>\n",
       "      <th>average_weighted_degree</th>\n",
       "      <th>average_clustering</th>\n",
       "      <th>num_components</th>\n",
       "      <th>largest_component_size</th>\n",
       "      <th>diameter</th>\n",
       "      <th>modularity</th>\n",
       "      <th>top5_degree_centrality</th>\n",
       "      <th>top5_betweenness_centrality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>23</td>\n",
       "      <td>235</td>\n",
       "      <td>0.928854</td>\n",
       "      <td>20.434783</td>\n",
       "      <td>436.956522</td>\n",
       "      <td>0.040114</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>0.028654</td>\n",
       "      <td>[(Mental Health &amp; Emotional Wellbeing, 1.0), (...</td>\n",
       "      <td>[(Allergy &amp; Immunology, 0.4310966810966811), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>United States</td>\n",
       "      <td>23</td>\n",
       "      <td>245</td>\n",
       "      <td>0.968379</td>\n",
       "      <td>21.304348</td>\n",
       "      <td>406.956522</td>\n",
       "      <td>0.031730</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>0.083552</td>\n",
       "      <td>[(Mental Health &amp; Emotional Wellbeing, 1.0), (...</td>\n",
       "      <td>[(Allergy &amp; Immunology, 0.33604989286807463), ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            label  nodes  edges   density  average_degree  \\\n",
       "0  United Kingdom     23    235  0.928854       20.434783   \n",
       "1   United States     23    245  0.968379       21.304348   \n",
       "\n",
       "   average_weighted_degree  average_clustering  num_components  \\\n",
       "0               436.956522            0.040114               1   \n",
       "1               406.956522            0.031730               1   \n",
       "\n",
       "   largest_component_size  diameter  modularity  \\\n",
       "0                      23         2    0.028654   \n",
       "1                      23         2    0.083552   \n",
       "\n",
       "                              top5_degree_centrality  \\\n",
       "0  [(Mental Health & Emotional Wellbeing, 1.0), (...   \n",
       "1  [(Mental Health & Emotional Wellbeing, 1.0), (...   \n",
       "\n",
       "                         top5_betweenness_centrality  \n",
       "0  [(Allergy & Immunology, 0.4310966810966811), (...  \n",
       "1  [(Allergy & Immunology, 0.33604989286807463), ...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "import community.community_louvain as community_louvain # import Louvain algo for community detection (calculate modularity)\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "base_dir = os.path.abspath(\"..\")\n",
    "file_path = os.path.join(base_dir, \"data\", \"bipartite_network\", \"general_patterns\", \"demographics\", \"country\", \"UK - US\")\n",
    "# output_path = os.path.join(base_dir, \"data\", \"bipartite_network\", \"general_patterns\", \"demographics\", \"ethnicity\")\n",
    "uk_file = os.path.join(file_path, \"cluster_co-occurrence_United Kingdom.json\")\n",
    "us_file = os.path.join(file_path, \"cluster_co-occurrence_United States.json\")\n",
    "\n",
    "def load_graph(json_path):\n",
    "    # Load edge list ( {\"source\": \"Hematology & Blood Disorders\", \"target\": \"Procedures, Surgeries & Medical Devices\", \"weight\": 2}\n",
    "    with open(json_path, \"r\", encoding = \"utf-8\") as f:\n",
    "        edges = json.load(f)\n",
    "        \n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Load the edge list into a weighted (undirected) graph\n",
    "    for entry in edges:\n",
    "        u = entry[\"source\"]  # source node (cluster)\n",
    "        v = entry[\"target\"]  # target node (cluster)\n",
    "        w = entry[\"weight\"]\n",
    "        G.add_edge(u, v, weight = w)\n",
    "        \n",
    "    return G\n",
    "\n",
    "def analyze_graph(G, label):\n",
    "    analysis = {}  # store graph's stats\n",
    "\n",
    "    # Basic stats\n",
    "    analysis[\"label\"] = label\n",
    "    analysis[\"nodes\"] = G.number_of_nodes()  # number of nodes (clusters) in a graph\n",
    "    analysis[\"edges\"] = G.number_of_edges()\n",
    "    analysis[\"density\"] = nx.density(G)  # [0-1] number of edges compared to fully connected graph (ratio of actual to all possible connections)\n",
    "    # 0 -> sparse; 1 -> fully connected\n",
    "\n",
    "    # Degree\n",
    "    degrees = dict(G.degree())  # number of neighbours of a node (cluster)\n",
    "    weighted_degrees = dict(G.degree(weight=\"weight\"))  # sum of egde weights\n",
    "    analysis[\"average_degree\"] = np.mean(list(degrees.values())) # how many (avg) other clusters is auch cluster connected to\n",
    "    analysis[\"average_weighted_degree\"] = np.mean(list(weighted_degrees.values()))  # how frequent (strong) are these connections btw clusters\n",
    "\n",
    "    # Connectedness of a graph (local (Clustering Coefficient) & globally (Connected Components)\n",
    "    \n",
    "    # Clustering Coefficient (how often neighbours of a node (cluster) are connected)\n",
    "    # If cluster A is connected to B and C, how likely that B and C are connected too?\n",
    "    clustering = nx.clustering(G, weight=\"weight\")\n",
    "    analysis[\"average_clustering\"] = np.mean(list(clustering.values()))  # avg across all nodes (cluster)\n",
    "\n",
    "    # Connected Components (cc) (see how many disconnected groups of clusters a graph has: a graph is unified or fragmented)\n",
    "    # reachability: in connected components, all the nodes are always reachable from each other\n",
    "    components = list(nx.connected_components(G))\n",
    "    largest_cc = max(components, key=len)  # largest cc\n",
    "    analysis[\"num_components\"] = len(components) # total number of cc ( 1 if everything is connected)\n",
    "    analysis[\"largest_component_size\"] = len(largest_cc) # how many nodes (clusters) are in the largest group\n",
    "    \n",
    "    # Diameter (the longest shortest path btw any 2 nodes (clusters) in a cc) -> computer diameter of the largest_cc\n",
    "    G_largest_cc = G.subgraph(largest_cc)\n",
    "    if nx.is_connected(G_largest_cc):\n",
    "        analysis[\"diameter\"] = nx.diameter(G_largest_cc)  # nx.diameter works on cc\n",
    "    else:\n",
    "        analysis[\"diameter\"] = \"Not connected\"\n",
    "    \n",
    "\n",
    "    # Modularity ( [0-1] how well a graph can be divided into communities (groups of closely connected nodes))\n",
    "    # 1 if the detected communities are well separated\n",
    "    # 0 if mixed up\n",
    "    partition = community_louvain.best_partition(G)  # Louvain algo to detect communities\n",
    "    analysis[\"modularity\"] = community_louvain.modularity(partition, G)\n",
    "    \n",
    "    # Centrlity Measures (nodes (clusters) of importance)\n",
    "    dc = nx.degree_centrality(G)  # nodes with most connections\n",
    "    bc = nx.betweenness_centrality(G, weight=\"weight\")  # what clusters serve as bridges between others (nodes most important for information flow)\n",
    "    \n",
    "    # Top (n=5) nodes for each centrality measure\n",
    "    analysis[\"top5_degree_centrality\"] = sorted(dc.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "    analysis[\"top5_betweenness_centrality\"] = sorted(bc.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "\n",
    "    # insted of lng which clustookier has the highest degree, we look into what pairs are most connected\n",
    "    # get the edge with the biggest weight and get the corresponding clusters of that \"heavy\" edge\n",
    "    # essentially: sort the original (json) list of edges by weights and get top (n=5) cluster-PAIRS (main point are pairs)\n",
    "\n",
    "    return analysis\n",
    "\n",
    "# Load JSON files (edges) as graphs\n",
    "G_uk = load_graph(uk_file)\n",
    "G_us = load_graph(us_file)\n",
    "\n",
    "# Analyze graphs\n",
    "uk_stats = analyze_graph(G_uk, \"United Kingdom\")\n",
    "us_stats = analyze_graph(G_us, \"United States\")\n",
    "\n",
    "summary_df = pd.DataFrame([uk_stats, us_stats])\n",
    "summary_df\n",
    "# summary_df = pd.DataFrame([uk_stats, us_stats])\n",
    "# summary_df[\"top5_degree_centrality\"].to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9e50de-f786-46b8-a83a-58a56b618210",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
